{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90df705",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Techniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795af37",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Main library: sklearn.feature_selection and sklearn.manifold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a0a8c",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "- When you want to visually explore the patterns in a high dimensional dataset.\n",
    "- Stands for t-distributed stochastic neighbor embedding\n",
    "- statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\n",
    "-  **HOW** t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullbackâ€“Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map.\n",
    "\n",
    "\n",
    "-  **PROS:**\n",
    "    - fd\n",
    "-  **CONS:**\n",
    "    - Only numeric data will be transformed. Non-numeric categorical data set can be used with one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afad230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "m = TSNE(learning_rate = 50) #high learning rates will be unstable and low learning rates will be more conservative\n",
    "\n",
    "tsne_features = m.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e6b32",
   "metadata": {},
   "source": [
    "#### Variance Thresholds\n",
    "\n",
    "- Since we want to select features with most amount of variance, we can select variance limits using **VarianceThreshold**\n",
    "- Usually larger values will also have larger variances, do to normalize, usually columns are divided by mean values before applying feature selection funciton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cae2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold =0.005) #setting threshold to be SD of atleast 1. Can be 0.005 after normalization\n",
    "\n",
    "sel.fit(df/ df.mean()) #dividing by mean to normalize\n",
    "\n",
    "mask = sel.get_support()\n",
    "\n",
    "df.loc[:,mask]\n",
    "\n",
    "#mask will be a boolean list with true and false value for every columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bfbe6",
   "metadata": {},
   "source": [
    "##### Missing values or high correlation:\n",
    "- Count if the missing values are more than 30% (or a randomly selected perc). If higher, feature might as well be dropped.\n",
    "- Find correlation between all features. For features with high level of correlation, one of them might be dropped. \n",
    "- **MUST BE CAREFUL!** If not sure that feature selection due to correlation will not result in loss of important information, use feature extraction instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Missing/NaN values'''\n",
    "mask = df.isna().sum() / len(df) < 0.30 #will provide boolean list of values with less than 30% missing/nan values. \n",
    "\n",
    "'''Correlation Matrix'''\n",
    "corr = df.corr().abs() #will give pairwise correlation matrix between all values. \n",
    "\n",
    "sns.pairplot(data=df, hue=#) #is an excellent way to check for correlation as well. \n",
    "             \n",
    "            #ORR\n",
    "\n",
    "mask_df = np.triu(np.ones_like(corr, dtype=bool)) #this mask will make an upper triangle of True values the same size as correlation matrix\n",
    "             \n",
    "sns.heatmap(data=df, mask=mask_df) #using mask here will hide upper half of matrix\n",
    "             \n",
    "####Filter for correlation###\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "tri_df = corr_df.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a927c95",
   "metadata": {},
   "source": [
    "## Feature Selection using model performance and complexity\n",
    "\n",
    "- Standardize the features after splitting into train and text groups using StandardScaler. Necessary in order to compare features. \n",
    "- Then use the model for prediction. If feature coefficient is low for a certain feature, you may drop it to reduce model complexity. \n",
    "- Each time a feature is dropped, coeff of other features will change so this is a recursive method\n",
    "\n",
    "- sklearn has model called **RFE - Recursive Feature Selection**. Will select features dependend on imporovement in model performance (based on train set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=LogisticRegression(),         #can be any estimator\n",
    "          n_features_to_select=2,\n",
    "          step = 10,                              #number of values to drop in each iteration\n",
    "          verbose=1) \n",
    "\n",
    "rfe.fit(X_train_std, y_train) #fit with standardized training set.\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036014d9",
   "metadata": {},
   "source": [
    "#### Tree Based Feature Selection\n",
    "- RandomForestClassifier() has feature_importances_ values for each feature. \n",
    "- Since its tree based aggregator model, no need for standardizing variables.\n",
    "- can also feed this to RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:,mask]\n",
    "\n",
    "'''OR'''\n",
    "\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2744120",
   "metadata": {},
   "source": [
    "#### Regularization:\n",
    "\n",
    "- **Lasso**: alpha or L1 Norm\n",
    "- **Ridge**: lamda or L2 Norm\n",
    "- Ridge and Lasso can be used for any algorithms involving weight parameters, including neural nets. \n",
    "Dropout is primarily used in any kind of neural networks e.g. ANN, DNN, CNN or RNN to moderate the learning.\n",
    "\n",
    "**We can use ensemble method and have multiple model vote on importance of features as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef16dd",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "#### PCA - Principle Component Analysis\n",
    "- Uses **eigen decomposition**\n",
    "- We are trying to find the \"intrinsic dimensions\" of a dataset. We can check using PCA with high variance\n",
    "- NOT recommended for categorical data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f44ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler and standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_std = scaler.fit_transform(df) #Always standardize data with var 1 and mean 0\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(df_std)\n",
    "\n",
    "# This changes the numpy array output back to a DataFrame\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4']) \n",
    "#this example had 4 components. Actual # of components will be min(n-1,p)\n",
    "\n",
    "# Inspect the explained variance ratio per component\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "#Understanding Components\n",
    "\n",
    "pca.components_[:,0] #1st Component\n",
    "\n",
    "#Pipelines\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
