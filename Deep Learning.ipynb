{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5760c556-dbf1-4182-8a94-b7148ff8f8bc",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "- Tensorflow has endges and nodes. Edges are tensors and nodes are operations. \n",
    "- You can define constants and variables. They can be zero dimensional, 1 or 2 dimensional vectors. \n",
    "- **add()** performs elementwise additional and both vectors must be same shape. \n",
    "- **multiply()** does element wise multiplication. Need to be same size/shape.\n",
    "- **matmul()** does matrix multiplication. matmul(A,B) - columns of A must be equal to rows of B. \n",
    "- **reduce_sum(A)** operator sums over all dimensions of A. reduce_sum(A,i) sums over ith dimension i.e. (0, 1 or 2)\n",
    "\n",
    "- **gradient()** operator calculates gradient of a function at a point. Can be used to find optimum values (min or max)\n",
    "- **reshape()** operator reshapes a tensor\n",
    "- **random()** fills tensor with entries popluated from a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41891c9e-4a1f-4ac2-8908-8eedf0ecceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model\n",
    "output = matmul(letter, model)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = reduce_sum(output)\n",
    "print(prediction.numpy())\n",
    "\n",
    "'''Linear Regression'''\n",
    "intercept = tf.Variable(0.1)\n",
    "slope = tf.Variable(0.1)\n",
    "\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features = size_log):\n",
    "\treturn intercept + slope*features\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features = size_log, targets = price_log):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "\treturn keras.losses.mse(targets,predictions)\n",
    "\n",
    "# Initialize an Adam optimizer\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Apply minimize, pass the loss function, and supply the variables\n",
    "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "\n",
    "\n",
    "# Plot data and regression line\n",
    "plot_results(intercept, slope\n",
    "             \n",
    "'''Deep Learning'''\n",
    "##### Low Level\n",
    "# From previous step\n",
    "bias1 = Variable(1.0)\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1, weights2)\n",
    "             \n",
    "############ High level\n",
    "             \n",
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1, activation='sigmoid')(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b8b2c-c261-451c-b4bd-0e79b03142b0",
   "metadata": {},
   "source": [
    "## Deeplearning with tensor flow:\n",
    "- High level approach uses API and simpler function calls such as: keras.layers.Dense(#of nodes, activation = 'function')\n",
    "- Low level approach uses linear algebriac functions such as: prod = matmul(inputs,weights), dense = keras.activations.sigmoid(prod)\n",
    "\n",
    "**Three most common activation functions**\n",
    "- Sigmoid: Primarily used in the outlput layer of binary classification problems. Between 0 and 1 (can define probability)\n",
    "- tanh: Like sigmoid in shape but between -1 and 1\n",
    "- ReLu (Rectified Linear Unit) : Mostly used in hidden layers. 0 to infinity. Never negative\n",
    "- leaky_relu: will let some negative values. \n",
    "- Softmax: When output layer had more than 2 classes. Ensures that the outputs will sum to one so we can interpret them as probabilities\n",
    "\n",
    "**Three most common minimization/optimization funcitons**\n",
    "- SGD: Stochastic Gradient Descent. learning rate needs to be optimized\n",
    "- RMS Propogation Optimizer: \n",
    "    - Root mean square. \n",
    "    - Allows different learning rates to each feature. Good for high dimensional problems.\n",
    "    - Lets build **momentum** and allows **decay**\n",
    "- Adam: \n",
    "    - Adaptive moment\n",
    "    -  Good first choice\n",
    "    -  Can set decay faster rate by lowering **beta1** parameter\n",
    "    -  Performs better (compared to RMS) with parameter default values which are commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2536e-3dc6-4020-a5cf-d9b75e2f8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = constant(inputs_arrays, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8139b7c-021d-48d4-bbb8-ee810b24826c",
   "metadata": {},
   "source": [
    "**Initialization** is important in deeplearning. Some methods include:\n",
    "- tf.random.normal([500,500]) - Using normal distribution to initialize variables for weights\n",
    "- The default dense layer function in keras (keras.layers.Dense(#, activation='func')) uses golrot uniform initializer\n",
    "- Can use kernel_initilizer parameter. For example to initialize zeros: keras.layers.Dense(#, activation='func', kernel_initilizer='zeros')\n",
    "\n",
    "To avoid overfitting, you can use **dropout** which randomly drops weights related to certain node during training operation.Will improve out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9f739-2b86-4b40-8173-5ec997bd1609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "w1 = Variable(random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(random.normal([7, 1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable([0])\n",
    "\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout rate of 0.25\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)\n",
    "\n",
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model using test features\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217eeb24-ee30-485e-87ef-6abdf8dc50d7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "source": [
    "### Defining a Sequential API\n",
    "Easiest way to make a neural network\n",
    "- from tf.keras.utils import to_categorical for one-hot encoding of the output column\n",
    "- model = keras.Sequential()\n",
    "- model.add('each layer')\n",
    "- model.add(layers.Dropout(0.#)) to drop some percentage of nodes to avoid overfitting\n",
    "- model.summary() to review architecture\n",
    "- model.compile('optimizer_unc', loss = 'loss_func') to optimize\n",
    "- model.get_weights() to get weights of the model at any given time\n",
    "\n",
    "Can use function form to use (2) models to predict the same set outputs instead of sequential. \n",
    "\n",
    "- model.fit(features, labels, epochs = #, validation_split = 0.#, callbacks= [early_stopping_monitor])\n",
    "\n",
    "### Fine tuning\n",
    "- Can fine tune models by defining your own optimizers. Using keras.optimizers, import SGD (constant learning rate) or modify other optimizers\n",
    "- **Callbacks** can use different functions to stop running model for different reasons. early stopping will stop if validation accuracy is not improving. \n",
    "    - from tensorflow.keras.callbacks import EarlyStopping\n",
    "- **Checkpoints** can be used to store parameters in between steps. Specially to store best values. \n",
    "    - from keras.callback import ModelCheckpoint\n",
    "\n",
    "### Saving and Loading a model to use it\n",
    "- model.save('model_name.h5')\n",
    "- To load a model, use method: from tf.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf737b99-45c3-421c-b174-ef753201b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Example'''\n",
    "# Define a Keras sequential model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "'''Using 2 models in functional form'''\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "\n",
    "'''Validation'''\n",
    "\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b73ed-5f07-45c9-b7de-aea425498233",
   "metadata": {},
   "source": [
    "### Estimators API\n",
    "- Much faster deployment\n",
    "- Define feature_column\n",
    "- Define input_function\n",
    "- Define estimator (use pre defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c417c-7fe5-4c68-8bb7-7a4d23a66d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing['price'])\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862f2da-4608-4d59-9341-06954b24784d",
   "metadata": {},
   "source": [
    "TensorFlow Hub\n",
    "- Has pretrained models\n",
    "- Transfer learning. Their models are trained on much larger samples so will give better predictions, specially for image models. \n",
    "\n",
    "TensorFlow Probability\n",
    "- More statistical distributions\n",
    "- Trainable distributions\n",
    "- Extended set of optimizers\n",
    "\n",
    "## Improving the model\n",
    "\n",
    "### Learning Curves\n",
    "- As number of epochs increase, learning curve decreases or accuracy curve increases.\n",
    "- They can be unstable in some areas at times. Variables like:\n",
    "    - Optimizer\n",
    "    - Learning Rate\n",
    "    - Batch size\n",
    "    - Network architecture\n",
    "    - weight initialization etc.\n",
    "\n",
    "#### Choosing the correct activation function\n",
    "- In general, start with relu, it generalizes well in most cases and train fast. Avoid sigmoids\n",
    "\n",
    "#### Batch size and normalization\n",
    "- Mini-batch is a set of samples\n",
    "- Usually, weights get updated at the end of every epoch. If we divide training data to mini-batches, they get updated more often. \n",
    "- Networks train faster on mini-batches\n",
    "- Less RAM is used, so we can use larger data size\n",
    "- Will need more iterations and have to find a good batch size. Batch size of 1 is stochastic gradient descent. \n",
    "- Larger dataset, larger batch size\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "**Dropout**\n",
    "- Randomly drops a subset of units during forward and backward propogation. This reduces sensitivity to noise in the data and the network to become overly correlated. \n",
    "- **from keras.layer import Dropout** \n",
    "- Add dropout at a layer and assign percentage of units to be dropped out from the last layer\n",
    "\n",
    "**Batch normalization**\n",
    "makes sure consecutive layers get normalized data as inputs to avoid issues with optimization and gradient descent. \n",
    "- Allows higher learning rate\n",
    "- Reduces dependence on weight initializations\n",
    "- Improves gradient flow\n",
    "- Limits internal covariance shift\n",
    "\n",
    "**Batch normalization and Dropout layers sometimes DO NOT WORK WELL TOGETHER**\n",
    "\n",
    "#### Review Model:\n",
    "- Use model history to review how it has improved accuracy during its training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f944ef0-ccb8-46ed-b3c3-bb1fc5a55235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial model \n",
    "weightsinit_weights = model.get_weights()\n",
    "# Lists for storing accuracies\n",
    "train_accs = []\n",
    "tests_accs = []\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    # Split a fraction according to train_size    \n",
    "    X_train_frac, _, y_train_frac, _ = train_test_split(X_train, y_train, train_size=train_size)\n",
    "    # Set model initial weights    \n",
    "    model.set_weights(initial_weights)\n",
    "    # Fit model on the training set fraction    \n",
    "    model.fit(X_train_frac, y_train_frac, \n",
    "              epochs=100,               #epoch is everytime the model goes through all the training samples\n",
    "              batch_size = 128          #size of each mini-batch. default 32, power of 2 is used. \n",
    "              verbose=0,                #how many messages come out during training a model\n",
    "              validation_data = (X_test, y_test),       #self explanatory\n",
    "              validation_split = 0.x,                   #Can be used instead of validation data\n",
    "              callbacks=[EarlyStopping(monitor='loss', patience=1), \n",
    "                         ModelCheckpoint('weights.hdf5', monitor='val_loss',save_best_only=True)]\n",
    "             )\n",
    "    \n",
    "    # Get the accuracy for this training set fraction    \n",
    "    train_acc = model.evaluate(X_train_frac, y_train_frac, verbose=0)[1]    \n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Get the accuracy on the whole test set    \n",
    "    test_acc = model.evaluate(X_test, y_test, verbose=0)[1]    \n",
    "    test_accs.append(test_acc)\n",
    "    print(\"Done with size: \", train_size)\n",
    "#######################REVIEW MODEL ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the history from the training object\n",
    "history = training.history\n",
    "\n",
    "# Plot the training loss \n",
    "plt.plot(history['loss'])\n",
    "# Plot the validation loss\n",
    "plt.plot(history['val_loss'])\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n",
    "\n",
    "#################### Batch Normalization #####################\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "model.add(BatchNormalization()) # can be added between layers as its own layer in Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b5dad-54d5-45f6-bb20-ba0ac7d9afa9",
   "metadata": {},
   "source": [
    "### Using RandomSearchCV from scikit-learn\n",
    "- Use Keras SKlearn wrapper: tensorflow.keras.wrappers.scikit_learn to make your model usable by sklearn\n",
    "- Use less number of epochs\n",
    "- Use randomsearch CV instead of grid search\n",
    "- Use smaller sample of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa433a-b3d2-43df-86e3-94122569cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Import sklearn wrapper from keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a model as a sklearn estimator\n",
    "model = KerasClassifier(build_fn=create_model) #defined a different create_model function below. Basically a new keras sequential model everytime the function is called\n",
    "\n",
    "# Import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Check how your keras model performs with 5 fold crossvalidation\n",
    "kfold = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "def create_model(nl=1,nn=256, optimizer, loss_func):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape=(2,), activation='relu'))\n",
    "    # Add as many hidden layers as specified in nl\n",
    "    for i inrange(nl):\n",
    "        # Layers have nn neurons        \n",
    "        model.add(Dense(nn, activation='relu'))\n",
    "        # End defining and compiling your model...\n",
    "        model.add(Dense(1, activation = 'sigmoid'))\n",
    "        model.compile(optimizer=optimizer, loss=loss_func)\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Define parameters, named just like in create_model()\n",
    "params = dict(nl=[1, 2, 9], \n",
    "              optimizer=['sgd', 'adam'], \n",
    "              epochs=3,\n",
    "              batch_size=[5, 10, 20], \n",
    "              activation=['relu','tanh']\n",
    "              nn=[128,256,1000])\n",
    "\n",
    "# Repeat the random search\n",
    "random_search = RandomizedSearchCV(model, params_dist=params, cv=3)\n",
    "random_search_results = random_search.fit(X, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Best: %f using %s\".format(random_search_results.best_score_,random_search_results.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4d9f2-5614-40b0-8f59-3835bb962d7c",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "An autoencoderis a neural network that is trained to attempt to copy its inputto its output.\n",
    "- They are designed to be unable to learn to copy perfectly. \n",
    "- Usually they are restricted in ways that allow them to copy only approximately, and to copy only input that resembles the training data. \n",
    "- Because the model is forced to prioritize which aspects of the input should be copied, it often learns useful properties of the data.\n",
    "\n",
    "#### Used for\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "- noise removal etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb9c74-e4fd-49a2-a636-99c93ba79d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Backend function\n",
    "    Can act like a layer would act at a certain stage of training\n",
    "'''\n",
    "# Import tensorflow.keras backend\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))\n",
    "\n",
    "'''Autoencoder'''\n",
    "\n",
    "\n",
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation (MINST)\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()\n",
    "\n",
    "#### CHECKING THE ENCODER\n",
    "\n",
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings \n",
    "encodings = encoder.predict(X_test_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63188763-a35a-4277-b7ed-49e0383273b9",
   "metadata": {},
   "source": [
    "## CNN\n",
    "- Convolution is a mathematical operation that preserves spatial relationships. \n",
    "- Convolution applies filter to each array to reduce dimensions\n",
    "- (Convolution + Relu) + Pooling -> Flattern -> Fully Connected -> Classification\n",
    "\n",
    "### Convolution: Kernels\n",
    "- Act as feature mapping operator over the input array(images).\n",
    "- Need to be flattened to integrate into fully connected network. \n",
    "- Conv2D is used for images. input shape to be same as image since we dont want to loose spatial relationships.\n",
    "- **Padding**\n",
    "    - Use \"zero-padding\" so that the size of the output of feature mapping is the same as input array.\n",
    "    - padding = \"valid\" - No zero padding is added. padding = \"same\" - zero padding is added. \n",
    "- **Strides**\n",
    "    - Step taken by Kernel in each step as it slides across the image.\n",
    "    - Stride = 1 is default. If stride is > 1, convolution output will be smaller. \n",
    " \n",
    " **O = ((Iâˆ’K+2P)/S)+1**\n",
    " \n",
    " I = size of the input\n",
    " \n",
    " K = size of the kernel\n",
    " \n",
    " P = size of the zero padding\n",
    " \n",
    " S = strides\n",
    " \n",
    "### Pooling:\n",
    "- Reduces the number of parameters after convolution. \n",
    "- Takes a small matrix, and replaces that matrix by one single value (example: maximum pooling)\n",
    "- Different types of pooling operations: MaxPool2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18b6e4-1019-4596-a3bc-b43289b2aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to import flatten and conv from keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "# Instantiate your model as usual\n",
    "model = Sequential()\n",
    "# Add a convolutional layer with 32 filters of size 3x3\n",
    "model.add(Conv2D(filters=32,                 \n",
    "                 kernel_size=3,                 \n",
    "                 input_shape=(28, 28, 1),                 \n",
    "                 activation='relu'))\n",
    "\n",
    "#Pooling layer to reduce number of parameters\n",
    "model.add(MaxPool2D(2))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add a dropout layer (explained earlier)\n",
    "model.add(Dropout(0.20)) #20% of units from previous layer\n",
    "\n",
    "# Flatten the output of the previous layer\n",
    "model.add(Flatten())\n",
    "# End this multiclass model with 3 outputs and softmax\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "'''Pre-processingimagesforResNet50'''\n",
    "\n",
    "# Import image from keras preprocessing\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# Import preprocess_input from tensorflow keras applications resnet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "# Turn it into an array\n",
    "img = image.img_to_array(img)\n",
    "# Expand the dimensions so that it's understood by our network:\n",
    "# img.shape turns from (224, 224, 3) into (1, 224, 224, 3)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "# Pre-process the img in the same way training images were\n",
    "img = preprocess_input(img)\n",
    "\n",
    "# Import ResNet50 and decode_predictions from tensorflow.keras.applications.resnet50\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, decode_predictions\n",
    "\n",
    "# Instantiate a ResNet50 model with imagenet weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on our img\n",
    "preds = model.predict(img)\n",
    "# Decode predictions and print it\n",
    "print('Predicted:', decode_predictions(preds, top=1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77e6de-3e2c-4fab-9dff-f70f12a2dd06",
   "metadata": {},
   "source": [
    "**Functional API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6b8abd-3fd1-43d3-8284-635a86fb5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/dense/output layers\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "input_tensor = Input(shape=(1,))\n",
    "output_tensor = Dense(1)(input_tensor)\n",
    "\n",
    "# Build the model\n",
    "from tensorflow.keras.models import Model\n",
    "model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(games_tourney_train[['seed_diff', 'pred']],\n",
    "  \t\t  games_tourney_train[['score_1', 'score_2']],\n",
    "  \t\t  verbose=True,\n",
    "  \t\t  epochs = 100,\n",
    "  \t\t  batch_size = 16384)\n",
    "\n",
    "# Import the plotting function\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n",
    "\n",
    "# Plot the model\n",
    "plot_model(model, to_file='model.png')\n",
    "\n",
    "# Display the image\n",
    "data = plt.imread('model.png')\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "# Print the model's weights\n",
    "print(model.get_weights())\n",
    "\n",
    "# Print the column means of the training data\n",
    "print(games_tourney_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffa279-8d14-455c-8bff-7a95b89d567b",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "- It is similar to PCA. A low dimensional representation of high dimensional data. \n",
    "- Used specially when there is high cardinality in data.\n",
    "- Embedding layer creates an additonal dimensional to the input. This is important for text and image data. For categorical data, need to flatten the embedded layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61fe8ed-eb43-43bf-8ce8-55350c2e7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from numpy import unique\n",
    "\n",
    "# Imports\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create an input layer for the team ID\n",
    "input_tensor = Input(shape=(1,))\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding_layer = Embedding(input_dim=n_teams,  #number of categorical variables\n",
    "                        output_dim=1,\n",
    "                        input_length=1,\n",
    "                        name='Team-Strength')\n",
    "\n",
    "# Lookup the input in the team strength embedding layer\n",
    "strength_lookup = embedding_layer(input_tensor)\n",
    "\n",
    "# Flatten the output\n",
    "strength_lookup_flat = Flatten()(strength_lookup)\n",
    "\n",
    "# Combine the operations into a single, re-usable model\n",
    "team_strength_model = Model(teamid_in, strength_lookup_flat, name='Team-Strength-Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de683ddc-b406-46fc-a2d0-9283b5b3a9ee",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "\n",
    "**Dropout**\n",
    "- Randomly drops a subset of units during forward and backward propogation. This reduces sensitivity to noise in the data and the network to become overly correlated. \n",
    "- **from keras.layer import Dropout** \n",
    "- Add dropout at a layer and assign percentage of units to be dropped out from the last layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7c00c-a615-4df1-bee4-27cc349f8a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
