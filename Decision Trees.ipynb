{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0397a6ef-dcfd-401c-93c9-bc2852846f09",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "- ***CART*** = Classification and Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7",
   "metadata": {
    "id": "bA5ajAmk7XH6"
   },
   "outputs": [],
   "source": [
    "#Import Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Import accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n",
    "\n",
    "#initialize decision tree model dt\n",
    "dt = DecisionTreeClassifier(max_depth=2, criterion='gini', min_samples_leaf = 0.1, random_state = SEED) \n",
    "'''\n",
    "max_depth = maximum depth of tree\n",
    "criterion = criteria for information gain {“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "min_samples_leaf = 0.1 means each leaf needs to have atleast 10% of training data. \n",
    "\n",
    "'''\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4be1b0-b5d8-4644-afdb-a652640c3f5e",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "- Train N different models with different algorithms from the same traning set and make a combined robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b8248-97f3-4a38-8026-93dd51248fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Initialize all models\n",
    "lr = LogisticRegression(random_state = SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state = SEED)\n",
    "\n",
    "#Define a list that has tuples: (\"Classifier_name\", Classifer)\n",
    "\n",
    "classifiers = [('Logistic Regression',lr),\n",
    "               ('KNearest Neighbors',knn),\n",
    "               ('Classification Tree',dt)]\n",
    "\n",
    "#Iterate over tuples\n",
    "\n",
    "for clf_name,clf in classifiers:\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(clf_name,accuracy_score(y_test,y_pred))\n",
    "    \n",
    "#Voting Classifier\n",
    "\n",
    "vc = VotingClassifier(esimators = classifiers, n_jobs=-1) #n-jobs = -1 will assign all CPUs cores to training this model. \n",
    "\n",
    "\n",
    "\n",
    "vc.fit(X_train,y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3dc419-6d43-4050-b5d6-dc0840b3e6f3",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "- Bootstrap data from same training set, make N different models with SAME algorithm and do voting to get better overall model\n",
    "- Out of Bag scoring technique gives accuracy similar to cross validation but FASTER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def5bbf-72fa-4c60-a0fc-ec8524331e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#initialize baseline model\n",
    "dt = DecisionTreeClassifier(max_depth = 8,min_samples_leaf = 0.16, random_state = SEED)\n",
    "\n",
    "#initialize bagging classifier\n",
    "bc = BaggingClassifier(base_estimator= dt, n_estimators=300, oob_score= True,n_jobs=-1)\n",
    "\n",
    "'''\n",
    "n_estimators = number of models(trees here) to be made and used for bagging\n",
    "\n",
    "oob_score = out of bag scoring technique\n",
    "'''\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_pred)\n",
    "oob_accuracy = bc.oob_score_                                            #OOB Score/Accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf067f7-76d6-4296-807a-23175940a57a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4671bfd-dac3-402c-a252-2c2e15374a8c",
   "metadata": {
    "collapsed": true,
    "executionTime": 621,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators= 40, min_samples_leaf=0.12, random_state=SEED)\n\n"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m----> 3\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.12\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[43mSEED\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SEED' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_features = 'sqrt',n_estimators= 400, min_samples_leaf=0.12, random_state=SEED)\n",
    "'''\n",
    "max_features = maximum number of features.\n",
    "                Default = 1\n",
    "                Other options: {“sqrt”, “log2”, None}\n",
    "'''\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "RMSE = MSE(y_test, y_pred)**0.5\n",
    "\n",
    "\n",
    "######################### FEATURE IMPORTANCE############################\n",
    "\n",
    "importances_rf = pd.Series(rf.feature_importances_,columns = X.columns)\n",
    "\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "sorted_importances_rf.plot(kind = 'barh', color='lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78f8b5-16b4-4f0a-a29e-88ab5e9a36dc",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "- ***Adaboost***: Adaptive Boosting\n",
    "-                     Each predictor pays more attention to instances wrongly predicted by its predecessor\n",
    "-                     Changes weights of instances\n",
    "-                     Each predictor is assigned alpha which depends on training error\n",
    "-                     Eta coefficient - learning rate. 0 < Eta < 1\n",
    "-                     Model 1 = M(X,y), Model 2 = M(W2,X,y), Model 3 = M(W3,X,y)... where alpha affects Weights\n",
    "                    \n",
    "#  \n",
    "                    \n",
    "- ***Gradient Boost (GB)***:  Uses CARTs           \n",
    "-                     Does not change weights of instances. Each predictor is trained using residual errors of its precessor as labels.\n",
    "-                     Residual r = y - y_hat\n",
    "-                     Eta - learning rate here is defined as Shrinkage\n",
    "-                     Model 1 = M(X,y), Model 2 = M(X, Etaxr1), Model 3 = M(X, Etaxr2)\n",
    "-                     y_pred = y1 + Eta x r1 + Eta x r2 + ...\n",
    "- ***Stochastic Gradient Boost (SGB)***: GB may lead to CARTs with same split points and features           \n",
    "-                     Each CART is trained on random subset of data, sampled without replacement\n",
    "-                     Features are also sampled without replacement to choose split points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efdade-6d4b-49a1-a47f-989324c6818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#initialize baseline model\n",
    "dt = DecisionTreeClassifier(max_depth = 1, random_state = SEED)\n",
    "\n",
    "#initialize bagging classifier\n",
    "adb_clf = AdaBoostClassifier(base_estimator= dt, n_estimators=100)\n",
    "\n",
    "adb_clf.fit(X_train, y_train)\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]                               #why predict_proba?? because roc_auc_score\n",
    "\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test,y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6199a-3719-442c-87aa-03005212502f",
   "metadata": {
    "collapsed": true,
    "executionTime": 74,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "from sklearn.ensemble import GradientBoostingRegressor\n\ngbt = GradientBoostingRegressor(n_estimators=300, max_depth = 1, random_state = SEED)\ngbt.fit(X_train, y_train)\ny_pred = gbt.predict(X_test)\n\nRMSE = MSE(y_test, y_pred)**0.5\n\n########################## Stochastic Gradient Boosting ##############################\n\ngbt = GradientBoostingRegressor(n_estimators=300, \n                                subsample= 0.8,                             #random sampling of instances\n                                max_features= 0.2,                          #random sampling of features\n                                max_depth = 1, \n                                random_state = SEED)"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SEED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingRegressor\n\u001b[0;32m----> 3\u001b[0m gbt \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[43mSEED\u001b[49m)\n\u001b[1;32m      4\u001b[0m gbt\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m gbt\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SEED' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth = 1, random_state = SEED)\n",
    "gbt.fit(X_train, y_train)\n",
    "y_pred = gbt.predict(X_test)\n",
    "\n",
    "RMSE = MSE(y_test, y_pred)**0.5\n",
    "\n",
    "########################## Stochastic Gradient Boosting ##############################\n",
    "\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, \n",
    "                                subsample= 0.8,                             #random sampling of instances 80%\n",
    "                                max_features= 0.2,                          #random sampling of features 20%\n",
    "                                max_depth = 1, \n",
    "                                random_state = SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc29a6-3438-4028-acfb-dbc0e5d5d3dd",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534a49c",
   "metadata": {
    "executionTime": 47,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "print(model.get_params()) # to see all the parameters of a model and decide which hyper parameter to tune"
   },
   "outputs": [],
   "source": [
    "print(model.get_params()) # to see all the parameters of a model and decide which hyper parameter to tune\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define params_dt\n",
    "params_dt = {'max_depth': [2,3,4],\n",
    "              'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]}\n",
    "\n",
    "# Instantiate grid_dt\n",
    "grid_dt = GridSeachCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       verbose = 1,                     #verbose means messages printed during fitting. Higher value = more messages\n",
    "                       n_jobs=-1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
