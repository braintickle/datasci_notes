{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15de7763",
   "metadata": {},
   "source": [
    "## Machine Learning with Timeseries Data\n",
    "\n",
    "### Feature Engineering\n",
    "- High amount of correlation in timeseries data. Need to conduct feature engineering on samples. \n",
    "- Some examples include:\n",
    " - Rolling Windows: Will give more information from data (mean, max and std) by smoothing data and making it positive. Specially used with audio data. \n",
    "   - For regressions, can use percentiles over each columns, date-based features etc. \n",
    " - Spectogram: a collection of windowed **Fourier transforms** over time. Specially used in ***audio*** data. \n",
    "   - Choose a window size and shape.\n",
    "   - At a timepoint, calculate FFT for that window\n",
    "   - aggregate results\n",
    "   - called **Short-Time Fourier Transform (STFT)**\n",
    "   - for spectogram, we square the values of STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculating Rolling Window Smoothing'''\n",
    "#Calculating a rolling window statistic\n",
    "#Smooth our data by taking the rolling mean in a window of 50 samples\n",
    "window_size = 50\n",
    "windowed = audio.rolling(window=window_size)\n",
    "audio_smooth = windowed.mean()\n",
    "\n",
    "#Two steps include:\n",
    "#1 - Rectification\n",
    "audio_rectified = audio.apply(np.abs)\n",
    "\n",
    "#2 - Smoothing\n",
    "audio_envelope = audio_rectified.rolling(50).mean()\n",
    "\n",
    "\n",
    "#For regression data:\n",
    "\n",
    "def percent_change(series):\n",
    "    # Collect all *but* the last value of this window, then the final value\n",
    "    previous_values = series[:-1]\n",
    "    last_value = series[-1]\n",
    "\n",
    "    # Calculate the % difference between the last value and the mean of earlier values\n",
    "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "    return percent_change\n",
    "\n",
    "# Apply your custom function and plot\n",
    "prices_perc = prices.rolling(20).aggregate(percent_change)\n",
    "\n",
    "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
    "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "\n",
    "# Define the features you'll calculate for each window\n",
    "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
    "\n",
    "# Calculate these features for your rolling window object\n",
    "features = prices_perc_rolling.agg(features_to_calculate)\n",
    "\n",
    "# Import partial from functools\n",
    "from functools import partial\n",
    "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
    "\n",
    "# Use a list comprehension to create a partial function for each quantile\n",
    "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
    "\n",
    "# Calculate each of these quantiles on the data using a rolling window\n",
    "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "features_percentiles = prices_perc_rolling.agg(percentile_functions)\n",
    "\n",
    "''' STFT '''\n",
    "# Import the stft function\n",
    "import librosa as lr\n",
    "from librosa.core import stft, amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Prepare the STFT\n",
    "HOP_LENGTH = 2**4\n",
    "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
    "\n",
    "# Convert into decibels\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ddc03",
   "metadata": {},
   "source": [
    "## Auto-correlation in Timeseries Data\n",
    "- Many times, the past data is a good predictor of future. More correlation leads to more smooth data\n",
    "- The time period used for making rolling windows should be chosen carefully. If too wide, the data points will have highamount of correlation and will not be i.i.d. \n",
    "- We can use time shifted models to see how smooth or correlation lies in the data. \n",
    "- Use df.shift(periods = x) where x is number of indices. \n",
    "- If the coefficients slowly go to zero as shift increases, the data might not be i.i.d. the more erratic, the better!\n",
    "\n",
    "Here are some steps you can take to address autocorrelation in your time series regression model:\n",
    "\n",
    "- **Differencing**: One of the most common ways to handle autocorrelation is to difference the time series data. This involves subtracting the previous value from the current value. This can help make the data stationary, which is a key assumption for regression. You may need to do this multiple times until the data is stationary.\n",
    "\n",
    "- **Lagged Variables**: You can include lagged values of the dependent variable or other relevant variables in your regression model. For example, if you are predicting a stock price, you can include lagged stock prices as predictors. This can help account for the autocorrelation in the data.\n",
    "\n",
    "- **Autoregressive Models**: Consider using autoregressive models, such as ARIMA (AutoRegressive Integrated Moving Average) or SARIMA (Seasonal ARIMA), which are specifically designed for handling autocorrelation in time series data. These models can be used alongside regression when appropriate.\n",
    "\n",
    "- **Moving Averages**: Another approach is to use moving averages to smooth the data and remove some of the noise. This can help in making the autocorrelation less pronounced.\n",
    "\n",
    "- Time Series Cross-Validation: When training and evaluating your regression model, be sure to use time series cross-validation techniques, such as time-based splitting, to account for the temporal structure of the data.\n",
    "\n",
    "- Residual Analysis: After building your model, analyze the residuals for autocorrelation. If there is still autocorrelation in the residuals, you may need to iterate and improve your model further.\n",
    "\n",
    "- **Advanced Models**: Consider more advanced time series modeling techniques like state-space models or Bayesian structural time series **(BSTS) models**, which can handle complex temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:\n",
    "# These are the \"time lags\"\n",
    "shifts = np.arange(1, 11).astype(int)\n",
    "\n",
    "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
    "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
    "\n",
    "# Convert into a DataFrame for subsequent use\n",
    "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
    "\n",
    "# Replace missing values with the median for each column\n",
    "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
    "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
    "\n",
    "# Fit the model\n",
    "model = Ridge()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ffecd",
   "metadata": {},
   "source": [
    "## Cross Validation with Time Series Data\n",
    "- Cannot just shuffle data since this will destroy the \"temporal\" nature of the data\n",
    "- Shuffling will result in some data from training set seeping into test set. So the metric will be incorrect.\n",
    "- **TimeSeriesSplit:** Special split function made for time series data for CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2de089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "fig, ax = plt.subplots()\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    ax.plot(tr, ii + y[tr])\n",
    "\n",
    "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd05562",
   "metadata": {},
   "source": [
    "### Stability\n",
    "- Change in relationship between X and y over time.\n",
    "- How to check for? Bootstrapping and looking at confidence intervals and checking cross val scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
    "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
    "    # Create our empty array to fill the results\n",
    "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
    "    for ii in range(n_boots):\n",
    "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
    "        random_sample = resample(data)\n",
    "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "        \n",
    "    # Compute the percentiles of choice for the bootstrapped means\n",
    "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
    "    return percentiles\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through CV splits\n",
    "n_splits = 100\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Create empty array to collect coefficients\n",
    "coefficients = np.zeros([n_splits, X.shape[1]])\n",
    "\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Fit the model on training data and collect the coefficients\n",
    "    model.fit(X[tr], y[tr])\n",
    "    coefficients[ii] = model.coef_\n",
    "    \n",
    "# Calculate a confidence interval around each coefficient\n",
    "bootstrapped_interval = bootstrap_interval(coefficients)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "ax.set(title='95% confidence interval for model coefficients')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
