{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa89cf85-8d07-48d3-86ea-a8658b16b3f6",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "- Tokenization library most commonly used: nltk\n",
    "    - from nltk.tokenize import\n",
    "        - sent_tokenize: tokenizes each sentances in a document\n",
    "        - word_tokenize: tokenizes each work in a document\n",
    "        - regexp_tokenize: tokenizes based on regular expressions. regexp_tokenize(text,pattern)\n",
    "        - TwitterTokenizer: for twitter\n",
    " \n",
    "- Preprocessing Steps:\n",
    "    - Includes Lemmatization, lowercasing, removing unwanted tokens.\n",
    "    - convert to lower words by using .lower() method after tokenization\n",
    "    - remove stop words: from nltk.corpus import stopwords.words('english')\n",
    "    - lemmatize: from nltk.stem import WordNetLemmatizer.lemmatize(word)\n",
    " \n",
    "Important tools \n",
    "- from collections import Counter: used for making bag of words \n",
    "- token.isalpha() returns only alphabetical tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7504e4-bc06-4ee5-8848-ba2710562139",
   "metadata": {},
   "source": [
    "## gensin library\n",
    "- Open source\n",
    "- Used to make word vectors and use bag of words to make dictionary and corpus\n",
    "- Also used for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dbd391-3d55-494f-a4d0-531ec0b4d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "############ TF-IDF ######################\n",
    "\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "      \n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bed05-85c3-44d7-8862-b4ad584770dd",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) [Stanford Java Libraries]\n",
    "- Gives context to words\n",
    "- Can tag words in document with description such as Persons, Places, Times, etc. \n",
    "- NLP library made by stanford. To use, need standard NLTK, the Stanford Java Libraries and some environment variables to help with integration\n",
    "- Important tool: nltk.pos_tag() tags each tokenized sentance as parts of speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4281ab-f74e-499c-a7ac-0770e88ead5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d712b-715c-4a2e-909a-914ac66efc31",
   "metadata": {},
   "source": [
    "## SpaCy library\n",
    "\n",
    "- Similar to gensim, also Open Source. Particularly used for creating NLP Pipelines to generate models and corpora.\n",
    "- Has extra tools like Displacy which makes interesting visualizations of parse matrices. Used to making interactive NER tags.\n",
    "- Has pre-trained word vectors used for entity detection- English, German and Chinese\n",
    "- Advantages:\n",
    "    - Easier pipeline creation\n",
    "    - Informal language corpora\n",
    "    - Different entity types than nltk\n",
    "    - Actively growing\n",
    "\n",
    "## polyglot\n",
    "- Has word embeddings for more than 130 languages. \n",
    "- Can be used for NER for multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8038be-4782-4f80-88c4-2f44bfd5d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher']) #Only interested in entities\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "######################## polyglot ###############################\n",
    "\n",
    "from polyglot.text import Text\n",
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636b006-f863-4af5-aa38-d3d523901c06",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "- MultinomialNB uses Naive Bayes which is a good place to start. \n",
    "- Bag-of-words or BoW: CountVectorizer\n",
    "- TF-IDF: Term frequency- inverse document frequency\n",
    "- n-grams: 2 words or 3 words in a sequence. Sequence is important. Also used to make embeddings. \n",
    "- Cosine_similaridy: Dot product/product of magnitudes - cosine_similarity() or linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26638bd-8ded-4ec9-844c-2d57f7773274",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BAG of Words ###\n",
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size = 0.33, random_state = 53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n",
    "\n",
    "###### TF-IDF#######\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n",
    "\n",
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n",
    "\n",
    "######################### RECCOMENDATION ###################\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix,tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
