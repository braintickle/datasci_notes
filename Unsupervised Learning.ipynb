{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed73d674",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300f451",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis\n",
    "- Gets rid of correlinearity between predictors. [STILL NEED TO NORMALIZE]\n",
    "- Uses **eigen decomposition**\n",
    "- We are trying to find the \"intrinsic dimensions\" of a dataset. We can check using PCA with high variance\n",
    "- NOT recommended for categorical data sets\n",
    "- scikit learn PCA does not support csr_matrix(only non zero entries are used). Used TruncatedSVD instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ab8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler and standardize the data\n",
    "scaler = StandardScaler(n_components = #) #if # is in [0,1], its variance explained, if its a number, its number of components\n",
    "df_std = scaler.fit_transform(df) #Always standardize data with var 1 and mean 0\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(df_std)\n",
    "\n",
    "# This changes the numpy array output back to a DataFrame\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4']) \n",
    "#this example had 4 components. Actual # of components will be min(n-1,p)\n",
    "\n",
    "######LOOK AT CORRELATION BETWEEN VALUES\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "correlation, pvalue = pearsonr(width,length)\n",
    "\n",
    "##### CHECKING FOR INTRINSIC DIMENSIONS\n",
    "\n",
    "features = range(model.n_components_)   #number of components\n",
    "variances = model.explained_variance_   #explained variances due to each components\n",
    "plt.bar(features, variances) #plot where it changes\n",
    "\n",
    "model.mean_ #finds mean of the model??\n",
    "\n",
    "# Get the first principal component: first_pc\n",
    "first_pc = model.components_[0,:]   #because principal components are shows only as an arrow in each row (to show direction) thus p dimensions\n",
    "\n",
    "# Create a PCA model with 2 components: \n",
    "model = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c1459",
   "metadata": {},
   "source": [
    "**NMF - Non Negative Matrix Factorization**\n",
    "- Interpretable (unlike PCA)\n",
    "- sample features must all be non-negative\n",
    "- Always need to specify components\n",
    "- To recreate sample, multiply features by components and add up. \n",
    "- for pictures: Greyscale & flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=2)\n",
    "model.fit(samples)\n",
    "features = model.transform(samples)\n",
    "components = model.components_ #features can be combined with components to recreate. Features X Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a792240",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "- t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\n",
    "\n",
    "- sklearn TSNE cannot fit data to a sample. Only combined fit_transform. Thus, cannot expand to include new samples.  \n",
    "\n",
    "- learning rate is a hyperparameter\n",
    "- axis of TSNE does not have any meaning. Only distance between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(learning_rate={int}) #assume learning rate as 100\n",
    "\n",
    "transformed = model.fit_transform(samples)\n",
    "\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "plt.scatter(xs, ys, c=actual_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3907cec",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "#### K Means Clustering\n",
    "- n_clusters = number of clusters\n",
    "- remembers centeriods of each cluster\n",
    "- inertia = average distance of points from their cluster means\n",
    "- centroid = center of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS STANDARDIZE YOUR DATA\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters={int})\n",
    "model.fit(samples)\n",
    "labels = model.predict(samples)\n",
    "labels_new = model.predict(samples_new)\n",
    "\n",
    "centers = model.cluster_centers_ #provides centers of each clusters/centroids\n",
    "inertia = model.inertia_  #provides inertias of clusters\n",
    "\n",
    "############MODEL EVALUATION################\n",
    "\n",
    "\n",
    "#########Cross Tables\n",
    "labels = model.fit_predict(samples) #fit and predict in one step on the same samples\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "####SCALING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(samples)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaled_samples = scaler.transform(samples)\n",
    "\n",
    "#Use pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters={int})\n",
    "\n",
    "pipeline = make_pipeline(scaler,kmeans)\n",
    "pipeline.fit(samples)\n",
    "pipeline.predict(sammples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26691b4c",
   "metadata": {},
   "source": [
    "**Hierarchial Clusturing**\n",
    "- linkage methods = complete, single, average, weighted, centroid\n",
    "- fcluster: Choosing height at which clustering stops is useful in classification\n",
    "\n",
    "\n",
    "In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55839e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "mergings = linkage(samples, method='complete')  #makes the cluster\n",
    "\n",
    "dendogram(mergings, labels=[list], leaf_rotation=90 , leaf_font_size = 6) #helps visualize data\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "labels = fcluster(mergings, 15, criterion='distance') #second arguement is distance/threshold\n",
    "\n",
    "#####YOU CAN DO THE SAME THING AS BEFORE i.e. crosstab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
