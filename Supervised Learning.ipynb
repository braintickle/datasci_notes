{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76bd6d6e",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a667c0-c5aa-419f-81b5-cd05aa3d9dfd",
   "metadata": {},
   "source": [
    "**Typical Model Syntax**\n",
    "- If using dataframe, always need 2-D array shape\n",
    "\n",
    "- df[['column']] gives Dataframe type while using df['column'] will give Series. Always use dataframe for scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f08ee-d313-4861-bd18-ac06efd36c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.module import Model\n",
    "model = Model()\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682095cc-7641-46a5-b39b-f9feac89d2ae",
   "metadata": {},
   "source": [
    "**Train vs Test Data Split**\n",
    "- Important for all models\n",
    "- random_state is seed for randombly splitting and reproducibility\n",
    "- test size is 30% below\n",
    "- stratify y so selection of each value of y matches proportion in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e1994-5909-4339-861c-ed9ad0c6adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3fccf",
   "metadata": {},
   "source": [
    "#### Pipelines\n",
    "- Multiple operations at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad333a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline example\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=2)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to dataframe and transform the data\n",
    "pc = pipe.fit_transform(df)\n",
    "\n",
    "#can access steps of pipeline using keywords\n",
    "pca = pipe['reducer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808fade-e230-4a29-bc81-1a6f18c5538c",
   "metadata": {},
   "source": [
    "**K Nearest Neighbours**\n",
    "- ALWAYS SCALE DATA FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fbba16-d2df-49fa-bf85-b291f8332325",
   "metadata": {
    "executionTime": 72,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5,weights={‘uniform’, ‘distance’or callabe array})\nknn.fit(X_train , y_train)\naccuracy = knn.score(X_test, y_test)"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5,weights={‘uniform’, ‘distance’or callabe array})\n",
    "knn.fit(X_train , y_train)\n",
    "accuracy = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf7ca5d-1619-4533-ad1b-b7d8d573d2bc",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91abc0-226a-4c95-8820-bf39173d6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "#####R-Squared#####\n",
    "R-square = reg.score(X_test, y_test)\n",
    "\n",
    "######RMSE######\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c374a-60ab-4c62-bbdf-7bb9f91cfac8",
   "metadata": {},
   "source": [
    "**Cross Validation**\n",
    "- shuffle arguement shuffles the data before dividing into n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ce196-d2ad-4fc8-a849-b948d39d4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state={int})\n",
    "cv_results = cross_val_score(model, X, y, cv=kf) ###output is R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413f8e2-13cf-415e-ae49-9c287491debf",
   "metadata": {},
   "source": [
    "Ridge Rigression (on Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505aa0f-e5e3-435f-b11d-084be8d7f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Ridge Rigression (on Linear Regression)####\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha={float})\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "####Lasso Rigression (on Linear Regression)####\n",
    "from sklearn.linear_model import Lasso\n",
    "#Same rest of the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c20e5a5-76c9-4dc5-97a4-ea387b894ad1",
   "metadata": {},
   "source": [
    "**Classification Metrics**\n",
    "- Confusion Matrix\n",
    "- Accuracy = TP + TN / All\n",
    "- Precision = True Positive (TP) / {TP + FP}\n",
    "- Recall = Sensitivity = TP / (TP + FN)\n",
    "- F1 Score = Recall x Precision / (Recall + Precision)\n",
    "- Support = # of instances within each class with their true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e89e3-c82f-4e2a-b83f-861a53d32fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#....make and train model, predict test values\n",
    "matrix = confusion_matrix(y_test, y_pred) #gives confusion matrix\n",
    "report = classification_report(y_test, y_pred) #gives f1 score, precision, recall and support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164f682-8745-4f59-a33f-8d37443d0f7d",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c695b-e4bf-42c9-a6e1-77912c7873ae",
   "metadata": {
    "executionTime": 713,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "from sklearn.linear_model import LogisticRegression\n#....make and train model (logreg) , predict test values\ny_pred_prob = logreg.predict_proba(X_test)[:,1] \n#probability of each instance to belong to a class i.e. p_k(X) "
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#....make and train model (logreg) , predict test values\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1] \n",
    "#probability of each instance to belong to a class i.e. p_k(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd926d6a-0eb4-4fc7-9848-ab0c70f2d564",
   "metadata": {},
   "source": [
    "**ROC Curve**\n",
    "- Changes as threshold for p_k(X) is changed. \n",
    "- tpr = true positive rate = TP/ (P) = Sensitivity = TP/(TP + FN)\n",
    "- fpr = false positive rate = FP/ (N) = FP/ (FP + TN)\n",
    "- ROC curve= tpr(y) vs fpr(x)\n",
    "- AUC = Area under curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2182b2-327a-40c7-9dfe-7f76b71c9b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) #all three values are calculated\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(y_test, y_pred_prob) #want it to be as close to 1 as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e61fd-0a47-4d19-9943-625b3c8df524",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning aka optimizing models\n",
    "- **GridSearchCV** = cross validates throughout a range of values. NOT SCALABLE\n",
    "- **RandomizesSearchCV** = set number of hyperparameters tested randomly using n_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc49e8e-b521-436d-a5aa-9ec171590982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state={int})\n",
    "\n",
    "#make a dictionary of parameters\n",
    "param_grid = {\"parameter1\": np.arange(start,stop,steps)\n",
    "            \"parameter2\": [\"value1\",\"method2\"]}\n",
    "ridge = Ridge() #initialize model\n",
    "ridge_cv = GridSeachCV(ridge, param_grid, cv=kf)\n",
    "\n",
    "ridge_cv.fit(X_train, y_train) #fit the model to training data\n",
    "\n",
    "best_values = ridge_cv.best_params_ #gives best values\n",
    "score = ridge_cv.best_score_ #best score corresponding to best params\n",
    "\n",
    "#########################################################################\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    ".\n",
    ".\n",
    ".\n",
    "ridge_cv = GridSeachCV(ridge, param_grid, cv=kf, n_iter=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d9a0b-72dd-4447-8e2c-741a91c159e9",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Use when:\n",
    "- Large number of training data (1000+) and few features (~100)\n",
    "- Categorical or numeric data\n",
    "\n",
    "DO NOT USE when:\n",
    "- Computer Vision/ NLP, etc\n",
    "- Small training sets\n",
    "- Training examples << features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bf064-c7cc-4c46-baba-e696a3ebbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost as xgb\n",
    "\n",
    "#Convert to DMatrix to use XGB\n",
    "data_dmatrix = xgb.DMatrix(data = data_db.iloc[:,:-1], label = data_db.iloc[:,-1]) \n",
    "\n",
    "#Defining Params\n",
    "params = {\"objective\":'binary:logistic', \"max_depth\":4}\n",
    "\n",
    "#Cross Validation\n",
    "cv_results = xgb.cv(dtrain = data_dmatrix, params = params, n_fold = 4, num_boost_round=10, metrics = \"error\", as_pandas = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c6382-ae61-46c9-983f-bedebd7bc247",
   "metadata": {},
   "source": [
    "Depending on type of base learner, one can use Scikit-learn based API or learning API from xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40a65f-1f7e-4a86-8795-e955762b503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Trees as Base Learners using Scikit-learn based API\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "pred = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "#Using Linear base learners: have to use learning API only\n",
    "\n",
    "DM_train = xgb.DMatrix(data = X_train, label = y_train) #need to convert to DMatrices\n",
    "DM_test = xgb.DMatrix(data = X_test, label = y_test)\n",
    "\n",
    "params = {\"booster\":'gblinear', \"objective\":\"reg:linear\"}\n",
    "\n",
    "xg_reg = xgb.train(dtrain = DM_train, params = params,num_boost_round=10) #Notice train instead of fit\n",
    "\n",
    "pred = xg_reg.predict(DM_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3636b-04db-4c08-b351-6607d9852274",
   "metadata": {},
   "source": [
    "Plotting trees and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee02dd7-8240-4bd2-847b-dff4a2eb30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\") #num_trees depends on value of num_boost_round (10 here)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5084e-52df-482b-b4ba-73397812d7cd",
   "metadata": {},
   "source": [
    "Using API learning to do cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7f345a-3ef7-458d-a360-3cba416ab909",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(dtrain = housing_dmatrix, \n",
    "                    params=params, # parameters such as learning-rate (eta), gamma, lambda, alpha, max_depth etc\n",
    "                    early_stopping_rounds=5, #stops early if no improvement in model performance is seen\n",
    "                    nfold=3, \n",
    "                    max_depth = 5,\n",
    "                    subsample = (%), # %samples used for each tree\n",
    "                    colsample_bytree = (%), # %features used for each tree\n",
    "                    num_boost_round=10, \n",
    "                    metrics='rmse', #metric\n",
    "                    as_pandas=True, # saves as pandas\n",
    "                    seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b24787",
   "metadata": {},
   "source": [
    "Using Grid Search CV or Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a141563",
   "metadata": {},
   "outputs": [],
   "source": [
    "######GRID SEARCH###########\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, \n",
    "                        param_grid = gbm_param_grid, \n",
    "                        scoring = 'neg_mean_squared_error', \n",
    "                        cv=4, \n",
    "                        verbose=1) #to understand the outputs better\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "########RANDOM SEARCH###########\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator = gbm,\n",
    "                                    param_distributions = gbm_param_grid, #distribution of parameter grid\n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_iter=5,    #number of iterations\n",
    "                                    cv=4, \n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cda33-5274-4101-8243-b68c9aab8aff",
   "metadata": {},
   "source": [
    "Using DataFrameMapper to use in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f3051-0d05-4395-bea5-6f2bcf76c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature], SimpleImputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, SimpleImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )\n",
    "\n",
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union()),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c9a13-fd36-4dcc-84aa-5be069cf0e55",
   "metadata": {},
   "source": [
    "# Word Vectorization and Weighting\n",
    "\n",
    "- Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. (Caring to Car)\t\n",
    "- Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. (Caring to Care)\n",
    "\n",
    "- Stopwords\n",
    "\n",
    "## TF-IDF or Term Frequency - Inverse Document Frequency\n",
    "\n",
    "High frequency words might be important but words like 'it' and 'the' need to be weighted down. \n",
    "***idf*** is used to weight down commonly occuring words\n",
    "\n",
    "- weight = tf x idf\n",
    "- tf = frequency of term **t** in a document **d** = log_10 (count (t,d) + 1)\n",
    "- df_t = document frequency = number of documents **t** occurs in.\n",
    "- idf = log_10 (**N**/**df_t**) where **N** is the number of documents in the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89943ea-8094-4c05-a75c-3f92c32e7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Vectorization and Weighting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vec = TfidfVectorizer()\n",
    "text_vec = tfid_vec.fit_transform(documents)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_vec.toarray(), y, stratify=y, random_state=42) \n",
    "# IMPORTANT have to convert the vector back into array for sklearn\n",
    "\n",
    "\n",
    "################# Finding Weights of words ################\n",
    "text_vec.vocabulary_ # gives the words in the vocabulary of the vector\n",
    "\n",
    "vocab = {v:k for k,v in text_vec.vocabulary_.items} #dictionary of all vocabulary\n",
    "\n",
    "text_vec[3].data # weights of the 4th row of the vector\n",
    "text_vec[3].indices # indices of the words whose weights are listed\n",
    "\n",
    "zipped_row = dict(zip(text_vec[3].indices,text_vec[3].data))\n",
    "\n",
    "#EXAMPLE:\n",
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3)) #top 3 words in row 9"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
