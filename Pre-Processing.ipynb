{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee629e1",
   "metadata": {},
   "source": [
    "# Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93231670",
   "metadata": {},
   "source": [
    "Main libraries: **sklearn.preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b3bf3",
   "metadata": {},
   "source": [
    "#### Working with categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making data categorical with order\n",
    "Cat_data1 = pd.Categorical(data, categories = ['A','B','C'], ordered=True)\n",
    "\n",
    "#using df.cat.method_name to be efficient\n",
    "df['column'].cat.categories #lists all categories in the column\n",
    "df['column'].cat.reorder_categories(new_categories=['cat1','cat2','cat3'], ordered=True, inplace=True)\n",
    "\n",
    "df['column'].cat.set_categories(new_categories=['cat1','cat2','cat3'], ordered=True, inplace=True) #anything not listed here will have NaN values\n",
    "\n",
    "df['column'].cat.add_categories(new_categories=['cat4','cat5']) #adds new categories\n",
    "df['column'].cat.remove_categories(removals=['cat4']) #removes categories. Will set cat4 to NaN values\n",
    "\n",
    "df['column'].cat.codes #makes codes as per alhpabetical order for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45214005",
   "metadata": {},
   "source": [
    "**Managing Categorical Variables**\n",
    "- Always encode AFTER splitting data\n",
    "- sklearn will not accept categorical variables\n",
    "- dummy variables 1 or 0 only. But can make multiple columns for many categories\n",
    "\n",
    "\n",
    "1. sklearn - LabelEncoder() - will give a number to each class in single column\n",
    "2. sklearn - OneHotEncoder() - one hot encoding - or binary encoding - one column per class, 0 or 1\n",
    "3. pandas - get_dummies()\n",
    "4. Using DictVectorizer() can do one hot encoding in one step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32255123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "\n",
    "X_train['categorical_column_le'] = le.fit_transform(X_train['categorical_column'])\n",
    "\n",
    "# Makes multiple columns for each categorical variable or dummy variables\n",
    "dummies = pd.get_dummies(df['column'], drop_first=True) \n",
    "#first value is dropped because we only p-1 features when all columns are zero, it is its own category\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3213e56",
   "metadata": {},
   "source": [
    "**Imputation and Pipelines** aka **Transformers**\n",
    "- Always SPLIT data first before imputing to reduce _data leakage_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac31789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "model = Model()\n",
    "\n",
    "steps = [(\"imputer\", imp_mean),\n",
    "        (\"model\", model)]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "pipeline.fit(X_train,y_train)\n",
    "pipeline.predict(X_test).....\n",
    "\n",
    "###################WHEN ONLY USING SIMPLEIMPUTE################\n",
    "f\n",
    "X_train =  imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb0cfa",
   "metadata": {},
   "source": [
    "##### Centering and Scaling\n",
    "- Standardizing: center around zero and variance of 1.\n",
    "- Min Zero Max One: Minus minimum and divide by range\n",
    "- Normalize: ??\n",
    "\n",
    "Always SPLIT data first before imputing to reduce data leakage\n",
    "\n",
    "##### StandardScaler\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "z = (x - u) / s\n",
    "where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.\n",
    "\n",
    "- **fit(X[, y, sample_weight])** - Compute the mean and std to be used for later scaling.\n",
    "\n",
    "- **fit_transform(X[, y])** - Fit to data, then transform it.\n",
    "\n",
    "- Always fit_transform the training set and ONLY transform the test set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#split data first then\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#'''Can also be used in pipelines'''\n",
    "\n",
    "# Build the steps\n",
    "steps = [(\"scaler\", StandardScaler()),\n",
    "         (\"logreg\", LogisticRegression())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create the parameter space\n",
    "parameters = {\"C\": np.linspace(0.001, 1.0, num=20)}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Instantiate the grid search object\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "# Fit to the training data\n",
    "cv.fit(X_train, y_train)\n",
    "print(cv.best_score_, \"\\n\", cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df806199",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "\n",
    "- TfidVectorizer from sklearn.feature_extraction.text will find the most important words from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec7dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####STRINGS#########\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search('\\d+\\.\\d+', length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking['Length'].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())\n",
    "\n",
    "########TEXT##############\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vec = TfidfVectorizer()\n",
    "text_vec = tfid_vec.fit_transform(documents)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_vec.toarray(), y, stratify=y, random_state=42) \n",
    "# IMPORTANT have to convert the vector back into array for sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
